{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d405621",
   "metadata": {},
   "source": [
    "## Random SVD\n",
    "\n",
    "In this notebook we focus on singular value approximation using randomised range finder approximation as presented in [1]. First we construct a NN to do digit recognition and we then use TFHessian to compute the full Hessian matrix. In particular first we define the layout of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955d11ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the training set there are 60000 images, of size 28x28.\n",
      "The data are labeled in the following categories, [5 0 4 ... 5 6 8]\n",
      "Number of trainable layers 4\n",
      "Number of weights trainable per layer 0, (784, 15)\n",
      "Number of weights trainable per layer 1, (15,)\n",
      "Number of weights trainable per layer 2, (15, 10)\n",
      "Number of weights trainable per layer 3, (10,)\n"
     ]
    }
   ],
   "source": [
    "#We import all the library we are gona need\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as spla\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "from random import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "mnist = tf.keras.datasets.mnist \n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(\"In the training set there are {} images, of size {}x{}.\"\n",
    "      .format(train_images.shape[0],train_images.shape[1],train_images.shape[2]))\n",
    "print(\"The data are labeled in the following categories, {}\"\n",
    "      .format(train_labels))\n",
    "#We normalize the dataset\n",
    "train_images = train_images/255.0\n",
    "test_images = test_images/255.0\n",
    "#We assemble the data set\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "#Keras Sequential allow us to place one layer after the other\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),#Vectorifying layer\n",
    "    tf.keras.layers.Dense(15, activation='sigmoid'),#128 weights layer\n",
    "    tf.keras.layers.Dense(10)#10 leyers weights.\n",
    "])\n",
    "\n",
    "print(\"Number of trainable layers {}\".format(len(model.trainable_weights)))\n",
    "print(\"Number of weights trainable per layer 0, {}\".format(model.trainable_weights[0].shape))\n",
    "print(\"Number of weights trainable per layer 1, {}\".format(model.trainable_weights[1].shape))\n",
    "print(\"Number of weights trainable per layer 2, {}\".format(model.trainable_weights[2].shape))\n",
    "print(\"Number of weights trainable per layer 3, {}\".format(model.trainable_weights[3].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499003d",
   "metadata": {},
   "source": [
    "We now proceed to train the neural network using a sthocastic gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3335da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c217c97d1d3248a287dd7e6216e3cfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7316\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "#Importing a lost function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "test_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "#Defining number of iterations\n",
    "epochs = 21\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (batch_train_images, batch_train_labels) in enumerate(train_dataset):\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(batch_train_images, training=True)  # Logits for this minibatch\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(batch_train_labels, logits)\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "                \n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(batch_train_labels, logits)\n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for batch_test_images, batch_test_labels in test_dataset:\n",
    "        test_logits = model(batch_test_images, training=False)\n",
    "        # Update val metrics\n",
    "        test_acc_metric.update_state(batch_test_labels, test_logits)\n",
    "    test_acc = test_acc_metric.result()\n",
    "    test_acc_metric.reset_states()\n",
    "print(\"Validation acc: %.4f\" % (float(test_acc),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499dc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(weights):\n",
    "    logits = model(batch_train_images, training=True) #Logits for this minibatch\n",
    "    # Compute the loss value for this minibatch.\n",
    "    loss_value = loss_fn(batch_train_labels, logits);\n",
    "    return loss_value;\n",
    "#We now use TFHessian to compute the Hessian of the NN.\n",
    "H = Hessian(Loss,model.trainable_weights,\"KERAS\")\n",
    "H.SwitchVerbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI the world is 1 process big !\n"
     ]
    }
   ],
   "source": [
    "matH= H.mat();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff99b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = spla.svdvals(matH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,randsigmas,_ = H.RandMatSVD(50,10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc66819",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Hessian's Singular Value rank=50,overfit=10\")\n",
    "plt.plot(randsigmas)\n",
    "plt.legend([r\"$\\sigma$\",r\"$\\overset{\\sim}{\\sigma}$\"])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title(\"Hessian's Singular Value rank=50,overfit=10\")\n",
    "plt.plot([abs(randsigmas[i]-sigmas[i]) for i in range(len(randsigmas))],\"--\")\n",
    "plt.legend([r\"$|\\sigma_j-\\overset{\\sim}{\\sigma}_j|$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da493565",
   "metadata": {},
   "source": [
    "We would like to make a case for using the random SVD alghorithm in second order optimization problem. Let us consider the Hessian produced during in the [Newton Method Notebook](./NewtonMethod.ipynb), and show that given their quick decay the randomised SVD is a good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea218bfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Hs = np.load(\"Hs.npy\")\n",
    "plt.figure()\n",
    "for H in Hs:\n",
    "    _, s,_ = np.linalg.svd(H)\n",
    "    plt.semilogy(s)\n",
    "plt.legend([r\"$H_\"+str(5*i)+\"$\" for i in range(len(Hs))])\n",
    "plt.title(\"Singular Value Hessian Regressian For Adult Problem\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
