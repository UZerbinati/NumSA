{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdf9204",
   "metadata": {},
   "source": [
    "## Federated Newton Learn\n",
    "In this section we present the algorithm named Federated Newton Learning (FEDNL) introduced in [2].\n",
    "\n",
    "| Flavour | tol | iteration |\n",
    "| :--- | --- | --- |\n",
    "| Vanilla Newton | 1e-4 | 4 |\n",
    "| Rank 1 Compression | 1e-4 | 10 |\n",
    "| Rank 1 Compression Diagonal Regularisation Identity Initial Hessian | 1e-4 | >50 |\n",
    "| Rank 1 Compression Diagonal Regularisation Null Initial Hessian | 1e-4 | >50 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda80be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a679ea",
   "metadata": {},
   "source": [
    "### Vanilla Newton\n",
    "First we reimplement the vanilla Newton method in the framework of FEDNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed472dee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcd260970ae4df9ad41859921563b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1]   8%|▊         | 4/50 [00:15<03:03,  3.98s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0]   8%|▊         | 4/50 [00:15<03:03,  3.98s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 1] Lost funciton at this iteration 0.3603573739528656  and gradient norm 0.15029960870742798\n",
       "(FedNL) [Iteration. 2] Lost funciton at this iteration 0.3375653922557831  and gradient norm 0.019174691289663315\n",
       "(FedNL) [Iteration. 3] Lost funciton at this iteration 0.3369176983833313  and gradient norm 0.0008387075504288077\n",
       "(FedNL) [Iteration. 4] Lost funciton at this iteration 0.33691510558128357  and gradient norm 4.160287971899379e-06\n",
       "Lost funciton at this iteration 0.35959863662719727  and gradient norm 0.02014993503689766\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.3142033517360687  and gradient norm 0.020152254030108452\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x)#,start=0*np.identity(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":119,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm; #A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%1 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x,H.comm),np.linalg.norm(grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02937f",
   "metadata": {},
   "source": [
    "### FEDNL Rank 1 Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb64355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50916d8ed9d4cc4b9867ac3cc4fe442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0]  20%|██        | 10/50 [00:35<02:22,  3.56s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1]  20%|██        | 10/50 [00:35<02:22,  3.56s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 1] Lost funciton at this iteration 0.3603573739528656  and gradient norm 0.15029960870742798\n",
       "(FedNL) [Iteration. 2] Lost funciton at this iteration 0.3409968614578247  and gradient norm 0.01779988408088684\n",
       "(FedNL) [Iteration. 3] Lost funciton at this iteration 0.3384902775287628  and gradient norm 0.007210684008896351\n",
       "(FedNL) [Iteration. 4] Lost funciton at this iteration 0.33764347434043884  and gradient norm 0.004843716975301504\n",
       "(FedNL) [Iteration. 5] Lost funciton at this iteration 0.33708077669143677  and gradient norm 0.0022912921849638224\n",
       "(FedNL) [Iteration. 6] Lost funciton at this iteration 0.3369518518447876  and gradient norm 0.0010489925043657422\n",
       "(FedNL) [Iteration. 7] Lost funciton at this iteration 0.3369239270687103  and gradient norm 0.0004203339631203562\n",
       "(FedNL) [Iteration. 8] Lost funciton at this iteration 0.3369181156158447  and gradient norm 0.0002461163676343858\n",
       "(FedNL) [Iteration. 9] Lost funciton at this iteration 0.3369161784648895  and gradient norm 0.00012412497017066926\n",
       "(FedNL) [Iteration. 10] Lost funciton at this iteration 0.3369154632091522  and gradient norm 7.398983871098608e-05\n",
       "Lost funciton at this iteration 0.3595954179763794  and gradient norm 0.020166713744401932\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.3142067790031433  and gradient norm 0.02014790289103985\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x)#,start=0*np.identity(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":1,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm; #A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%1 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x,H.comm),np.linalg.norm(grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31ff3a",
   "metadata": {},
   "source": [
    "### FEDNL With Diagonal Regularisation And Different Initial Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44cb3585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4927eaf6698f42b884b5e54f4e1a47c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] 100%|██████████| 50/50 [02:45<00:00,  3.31s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] 100%|██████████| 50/50 [02:45<00:00,  3.31s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 1] Lost funciton at this iteration 0.595215380191803  and gradient norm 0.47412794828414917\n",
       "(FedNL) [Iteration. 2] Lost funciton at this iteration 0.48810920119285583  and gradient norm 0.20943771302700043\n",
       "(FedNL) [Iteration. 3] Lost funciton at this iteration 0.4362362027168274  and gradient norm 0.11586670577526093\n",
       "(FedNL) [Iteration. 4] Lost funciton at this iteration 0.41390082240104675  and gradient norm 0.08477818965911865\n",
       "(FedNL) [Iteration. 5] Lost funciton at this iteration 0.3987269699573517  and gradient norm 0.0656685009598732\n",
       "(FedNL) [Iteration. 6] Lost funciton at this iteration 0.3850094676017761  and gradient norm 0.051555052399635315\n",
       "(FedNL) [Iteration. 7] Lost funciton at this iteration 0.3753131330013275  and gradient norm 0.041504524648189545\n",
       "(FedNL) [Iteration. 8] Lost funciton at this iteration 0.36772263050079346  and gradient norm 0.034011147916316986\n",
       "(FedNL) [Iteration. 9] Lost funciton at this iteration 0.3621474504470825  and gradient norm 0.028476238250732422\n",
       "(FedNL) [Iteration. 10] Lost funciton at this iteration 0.35779914259910583  and gradient norm 0.024184321984648705\n",
       "(FedNL) [Iteration. 11] Lost funciton at this iteration 0.35446950793266296  and gradient norm 0.020877160131931305\n",
       "(FedNL) [Iteration. 12] Lost funciton at this iteration 0.351872980594635  and gradient norm 0.01824893243610859\n",
       "(FedNL) [Iteration. 13] Lost funciton at this iteration 0.3496452271938324  and gradient norm 0.015923520550131798\n",
       "(FedNL) [Iteration. 14] Lost funciton at this iteration 0.34779930114746094  and gradient norm 0.013942483812570572\n",
       "(FedNL) [Iteration. 15] Lost funciton at this iteration 0.34630343317985535  and gradient norm 0.012327508069574833\n",
       "(FedNL) [Iteration. 16] Lost funciton at this iteration 0.3450678586959839  and gradient norm 0.010984799824655056\n",
       "(FedNL) [Iteration. 17] Lost funciton at this iteration 0.3440246284008026  and gradient norm 0.00984558742493391\n",
       "(FedNL) [Iteration. 18] Lost funciton at this iteration 0.3431442975997925  and gradient norm 0.008875180967152119\n",
       "(FedNL) [Iteration. 19] Lost funciton at this iteration 0.34239742159843445  and gradient norm 0.008040432818233967\n",
       "(FedNL) [Iteration. 20] Lost funciton at this iteration 0.3417515754699707  and gradient norm 0.007312088273465633\n",
       "(FedNL) [Iteration. 21] Lost funciton at this iteration 0.3411938548088074  and gradient norm 0.006675973534584045\n",
       "(FedNL) [Iteration. 22] Lost funciton at this iteration 0.34071072936058044  and gradient norm 0.006116477306932211\n",
       "(FedNL) [Iteration. 23] Lost funciton at this iteration 0.340285986661911  and gradient norm 0.005619917996227741\n",
       "(FedNL) [Iteration. 24] Lost funciton at this iteration 0.339911550283432  and gradient norm 0.005175355356186628\n",
       "(FedNL) [Iteration. 25] Lost funciton at this iteration 0.3395807147026062  and gradient norm 0.004775238689035177\n",
       "(FedNL) [Iteration. 26] Lost funciton at this iteration 0.33928555250167847  and gradient norm 0.004412214737385511\n",
       "(FedNL) [Iteration. 27] Lost funciton at this iteration 0.33902254700660706  and gradient norm 0.004081723280251026\n",
       "(FedNL) [Iteration. 28] Lost funciton at this iteration 0.33878737688064575  and gradient norm 0.003779197810217738\n",
       "(FedNL) [Iteration. 29] Lost funciton at this iteration 0.33857667446136475  and gradient norm 0.003501043189316988\n",
       "(FedNL) [Iteration. 30] Lost funciton at this iteration 0.3383881747722626  and gradient norm 0.003244989551603794\n",
       "(FedNL) [Iteration. 31] Lost funciton at this iteration 0.3382195830345154  and gradient norm 0.0030086569022387266\n",
       "(FedNL) [Iteration. 32] Lost funciton at this iteration 0.3380691409111023  and gradient norm 0.0027906945906579494\n",
       "(FedNL) [Iteration. 33] Lost funciton at this iteration 0.33793482184410095  and gradient norm 0.0025882828049361706\n",
       "(FedNL) [Iteration. 34] Lost funciton at this iteration 0.33781489729881287  and gradient norm 0.0024012327194213867\n",
       "(FedNL) [Iteration. 35] Lost funciton at this iteration 0.3377077877521515  and gradient norm 0.0022275573574006557\n",
       "(FedNL) [Iteration. 36] Lost funciton at this iteration 0.33761218190193176  and gradient norm 0.0020656713750213385\n",
       "(FedNL) [Iteration. 37] Lost funciton at this iteration 0.3375266194343567  and gradient norm 0.0019143648678436875\n",
       "(FedNL) [Iteration. 38] Lost funciton at this iteration 0.3374503254890442  and gradient norm 0.001772492891177535\n",
       "(FedNL) [Iteration. 39] Lost funciton at this iteration 0.3373822867870331  and gradient norm 0.0016399776795879006\n",
       "(FedNL) [Iteration. 40] Lost funciton at this iteration 0.33732181787490845  and gradient norm 0.0015160831389948726\n",
       "(FedNL) [Iteration. 41] Lost funciton at this iteration 0.33726826310157776  and gradient norm 0.0014002007665112615\n",
       "(FedNL) [Iteration. 42] Lost funciton at this iteration 0.33722081780433655  and gradient norm 0.0012917278800159693\n",
       "(FedNL) [Iteration. 43] Lost funciton at this iteration 0.33717894554138184  and gradient norm 0.0011904510902240872\n",
       "(FedNL) [Iteration. 44] Lost funciton at this iteration 0.3371422588825226  and gradient norm 0.0010961189400404692\n",
       "(FedNL) [Iteration. 45] Lost funciton at this iteration 0.33711013197898865  and gradient norm 0.0010082934750244021\n",
       "(FedNL) [Iteration. 46] Lost funciton at this iteration 0.3370820879936218  and gradient norm 0.0009264704422093928\n",
       "(FedNL) [Iteration. 47] Lost funciton at this iteration 0.33705759048461914  and gradient norm 0.0008504096185788512\n",
       "(FedNL) [Iteration. 48] Lost funciton at this iteration 0.3370363712310791  and gradient norm 0.0007797974394634366\n",
       "(FedNL) [Iteration. 49] Lost funciton at this iteration 0.33701804280281067  and gradient norm 0.0007141471141949296\n",
       "Lost funciton at this iteration 0.3599795997142792  and gradient norm 0.02023960091173649\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.3139961063861847  and gradient norm 0.020151183009147644\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x,start=0*np.identity(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":1,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%1 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x,H.comm),np.linalg.norm(grad)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
