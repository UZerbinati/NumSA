{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbe0095",
   "metadata": {},
   "source": [
    "## Newton Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413bc68",
   "metadata": {},
   "source": [
    "### Quadratic Energy Minimization\n",
    "We are minimizing the following energy functional, using a Netwon method based on the TF Hessian library.\n",
    "$$J(x,y) = x^2y^2+xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bbcb1a",
   "metadata": {},
   "source": [
    "which is the unique stationary point of $\\nabla J$ given the fact that $J(x,y)$ is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1dff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import all the library we are gona need\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032b70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost funciton at this iteration [0.0101], gradient norm 0.1442497819662094 and is achived at point <tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[0.1],\n",
      "       [0.1]], dtype=float32)>\n",
      "Computed the  first gradient ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:00, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed search search diratcion ...\n",
      "Entering the Netwton optimization loop\n",
      "Lost funciton at this iteration [1.4239612e-05]  and gradient norm 0.1442497819662094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:00<00:00, 15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost funciton at this iteration [8.984099e-16], gradient norm 9.324332417293135e-09 and is achived at point <tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[1.8767974e-08],\n",
      "       [4.7869307e-08]], dtype=float32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "itmax = 10; # Number of epoch.\n",
    "tol = 1e-8\n",
    "step_size = 1; #Learning rate\n",
    "def Loss(x):\n",
    "    return (x[0]**2)*(x[1]**2)+x[0]*x[1];\n",
    "#Defining the Hessian class for the above loss function in x0\n",
    "x = tf.Variable(0.1*np.ones((2,1),dtype=np.float32))\n",
    "H =  Hessian(Loss,x)\n",
    "grad = H.grad().numpy();\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and is achived at point {}\"\n",
    "      .format(Loss(x),np.linalg.norm(grad),x));\n",
    "print(\"Computed the  first gradient ...\")\n",
    "q = H.pCG(grad,1,1,tol=tol,itmax=100);\n",
    "print(\"Computed search search diratcion ...\")\n",
    "print(\"Entering the Netwton optimization loop\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    if it%50 == 0:\n",
    "        print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        print(\"Lost funciton at this iteration {}, gradient norm {} and is achived at point {}\"\n",
    "      .format(Loss(x),np.linalg.norm(grad),x));\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    q = H.pCG(grad,1,1,tol=tol,itmax=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9db1d",
   "metadata": {},
   "source": [
    "### Regression\n",
    "Our objective is to minimize the function:\n",
    "\\begin{equation}\n",
    "    f(\\vec{x}) = \\frac{1}{m} \\sum_{i=1}^m \\log\\Bigg(1+\\exp\\Big(-b_j \\vec{a_j}^T\\vec{x}\\Big)\\Bigg)\\qquad for \\; x \\in \\mathbb{R}^d\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199b43a",
   "metadata": {},
   "source": [
    "where $d$ is the feature number and $\\vec{a}_j$ are the data while $b_j$ are the labels.\n",
    "Now we would like to this applying the newton method to find a point that minimize such a function. This is possible because since $f$ is convex, all stationary points are minimizers and we search for the \"roots\" of the equation $\\nabla f=0$.\n",
    "The newton method we implement is of the form,\n",
    "\\begin{equation}\n",
    "    \\vec{x}_{n+1} = \\vec{x}_n -\\gamma Hf(\\vec{x}_n)^{-1}\\nabla f(\\vec{x}_n)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8957d",
   "metadata": {},
   "source": [
    "where $\\gamma$ is the step size.\n",
    "We solve the system $Hf(\\vec{x}_n)q=\\nabla f(\\vec{x}_n)$ using the CG method where as a preconditioned we have taken a the inverse of $Hf(\\vec{x}_n)$ computed using the random SVD presented in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853a605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import all the library we are gona need\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b73e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1605, 119) (1605,)\n"
     ]
    }
   ],
   "source": [
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215af61a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:00<00:00, 233.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed the  first gradient ...\n",
      "Computed search search diratcion ...\n",
      "Lost funciton at this iteration 0.9243690371513367  and gradient norm 1.3872039318084717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:00<00:00, 226.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost funciton at this iteration 0.3988267183303833  and gradient norm 0.07041343301534653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 224.23it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost funciton at this iteration 0.37019962072372437  and gradient norm 0.041932351887226105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:07<06:03,  3.70s/it]"
     ]
    }
   ],
   "source": [
    "#Setting the parameter of this run, we will use optimization nomeclature not ML one.\n",
    "itmax = 100; # Number of epoch.\n",
    "tol = 1e-4\n",
    "step_size = 0.2; #Learning rate\n",
    "Err = [];\n",
    "\"\"\"\n",
    "#Old Lost Function, intesead using Stefano's.\n",
    "#Defining the Loss Function\n",
    "def Loss(x):\n",
    "    S = tf.Variable(0.0);\n",
    "    for j in range(X.shape[0]):\n",
    "        a = tf.constant((X[j,:].todense().reshape(119,1)),dtype=np.float32);\n",
    "        b = tf.constant(Y[j],dtype=np.float32)\n",
    "        a = tf.reshape(a,(119,1));\n",
    "        x = tf.reshape(x,(119,1));\n",
    "        dot = tf.matmul(tf.transpose(a),x);\n",
    "        S = S+tf.math.log(1+tf.math.exp(-b*dot))\n",
    "    S = (1/X.shape[0])*S;\n",
    "    return S;\n",
    "\"\"\"\n",
    "tfX = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfY = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "\n",
    "\n",
    "#Defining the Loss Function\n",
    "def Loss(x):\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX.shape[0])\n",
    "    return S\n",
    "\n",
    "#Defining the Hessian class for the above loss function in x0\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "H =  Hessian(Loss,x)\n",
    "grad = H.grad().numpy();\n",
    "print(\"Computed the  first gradient ...\")\n",
    "q = grad #H.pCG(grad,10,2,tol=1e-3,itmax=10);\n",
    "print(\"Computed search search diratcion ...\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    if it%50 == 0:\n",
    "        print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    q = grad #H.pCG(grad,10,2,tol=1e-3,itmax=10);\n",
    "itmax = 100; # Number of epoch.\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    Err = Err + [np.linalg.norm(grad)];\n",
    "    if it%10 == 0:\n",
    "        print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    q = H.pCG(grad,65,10,tol=1e-4,itmax=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1feb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x),np.linalg.norm(grad)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da71096",
   "metadata": {},
   "source": [
    "### Quasi-Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import all the library we are gona need\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe050d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de9805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Setting the parameter of this run, we will use optimization nomeclature not ML one.\n",
    "itmax = 100; # Number of epoch.\n",
    "tol = 1e-4\n",
    "step_size = 0.2; #Learning rate\n",
    "Err = [];\n",
    "Hs = [];\n",
    "Rsigmas50 = [];\n",
    "Rsigmas80 = [];\n",
    "\"\"\"\n",
    "#Old Lost Function, intesead using Stefano's.\n",
    "#Defining the Loss Function\n",
    "def Loss(x):\n",
    "    S = tf.Variable(0.0);\n",
    "    for j in range(X.shape[0]):\n",
    "        a = tf.constant((X[j,:].todense().reshape(119,1)),dtype=np.float32);\n",
    "        b = tf.constant(Y[j],dtype=np.float32)\n",
    "        a = tf.reshape(a,(119,1));\n",
    "        x = tf.reshape(x,(119,1));\n",
    "        dot = tf.matmul(tf.transpose(a),x);\n",
    "        S = S+tf.math.log(1+tf.math.exp(-b*dot))\n",
    "    S = (1/X.shape[0])*S;\n",
    "    return S;\n",
    "\"\"\"\n",
    "tfX = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfY = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "\n",
    "\n",
    "#Defining the Loss Function\n",
    "def Loss(x):\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX.shape[0])\n",
    "    return S\n",
    "#Defining the Hessian class for the above loss function in x0\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "H =  Hessian(Loss,x)\n",
    "grad = H.grad().numpy();\n",
    "print(\"Computed the  first gradient ...\")\n",
    "q = grad #H.pCG(grad,10,2,tol=1e-3,itmax=10);\n",
    "print(\"Computed search search diratcion ...\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    if it%50 == 0:\n",
    "        print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    q = grad #H.pCG(grad,10,2,tol=1e-3,itmax=10);\n",
    "itmax = 100; # Number of epoch.\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    Err = Err + [np.linalg.norm(grad)];\n",
    "    if it%5 == 0:\n",
    "        print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x),np.linalg.norm(grad)));\n",
    "        Hs = Hs + [H.mat()];\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    U, s, Vt = H.RandMatSVD(50,10)\n",
    "    if it%5 == 0:\n",
    "        Rsigmas50 = Rsigmas50 + [s];\n",
    "    U, s, Vt = H.RandMatSVD(80,10)\n",
    "    if it%5 == 0:\n",
    "        Rsigmas80 = Rsigmas80 + [s];\n",
    "    q = (Vt.transpose()@np.linalg.inv(np.diag(s))@U.transpose())@grad;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Hs\",Hs)\n",
    "print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x),np.linalg.norm(grad)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd6ffc",
   "metadata": {},
   "source": [
    "We would like to make a case for using the random SVD alghorithm in second order optimization problem. Let us consider the Hessian produced during in the in pevious example and show that given their quick decay the randomised SVD is a good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e26f48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Hs = np.load(\"Hs.npy\")\n",
    "plt.figure()\n",
    "for H in Hs:\n",
    "    _, s,_ = np.linalg.svd(H)\n",
    "    plt.semilogy(s)\n",
    "plt.legend([r\"$H_{\"+str(5*i)+\"}$\" for i in range(len(Hs))])\n",
    "plt.title(\"Singular Value Hessian Regressian For Adult Problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666476b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_, s,_ = np.linalg.svd(Hs[4])\n",
    "plt.semilogy(s)\n",
    "plt.semilogy(Rsigmas50[4],\"--\")\n",
    "plt.semilogy(Rsigmas80[4],\"--\")\n",
    "plt.legend([r\"$H_{4}$\",r\"$\\overset{\\sim}{H}_4^{50}$\",r\"$\\overset{\\sim}{H}_4^{80}$\"])\n",
    "plt.title(\"Singular Value Hessian Regressian For Adult Problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d3aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cf36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "from mpi4py import MPI\n",
    "import dsdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "itmax = 100; # Number of epoch.\n",
    "tol = 1e-4\n",
    "step_size = 0.8; #Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0])\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9411bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "H =  Hessian(Loss,x)\n",
    "grad = H.grad().numpy();\n",
    "q = grad\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    if it%50 == 0:\n",
    "        print(\"[Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,Loss(x,H.comm),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    q = grad #H.pCG(grad,10,2,tol=1e-3,itmax=10);\n",
    "    \n",
    "itmax = 200\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    if it%5 == 0:\n",
    "        print(\"[Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,Loss(x,H.comm),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    U, s, Vt = H.RandMatSVD(65,10)\n",
    "    q = (Vt.transpose()@np.linalg.inv(np.diag(s))@U.transpose())@grad;\n",
    "print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x,H.comm),np.linalg.norm(grad)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bb770",
   "metadata": {},
   "source": [
    "### Federated Newton Learn\n",
    "In this section we present the algorithm named Federated Newton Learning (FEDNL) introduced in [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0ecf958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4c53e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a1ae6eecca4c9e863a4a9ecb5ad1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0])\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f85f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:0] 100%|██████████| 100/100 [00:00<00:00, 281.79it/s]\n",
       " 12%|█▎        | 25/200 [01:39<11:31,  3.95s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] 100%|██████████| 100/100 [00:00<00:00, 279.28it/s]\n",
       " 13%|█▎        | 26/200 [01:41<11:29,  3.96s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [Iteration. 0] Lost funciton at this iteration 0.9227030873298645  and gradient norm 1.3872549533843994\n",
       "[Iteration. 50] Lost funciton at this iteration 0.4135685861110687  and gradient norm 0.06812936067581177\n",
       "[Iteration. 0] Lost funciton at this iteration 0.3849295675754547  and gradient norm 0.04145266115665436\n",
       "[Iteration. 5] Lost funciton at this iteration 0.3779388666152954  and gradient norm 0.03542470932006836\n",
       "[Iteration. 10] Lost funciton at this iteration 0.37261834740638733  and gradient norm 0.031228117644786835\n",
       "[Iteration. 15] Lost funciton at this iteration 0.3683735728263855  and gradient norm 0.028081871569156647\n",
       "[Iteration. 20] Lost funciton at this iteration 0.3648774027824402  and gradient norm 0.025608040392398834\n",
       "[Iteration. 25] Lost funciton at this iteration 0.36192959547042847  and gradient norm 0.0236001405864954\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [Iteration. 0] Lost funciton at this iteration 0.9254758358001709  and gradient norm 1.3883233070373535\n",
       "[Iteration. 50] Lost funciton at this iteration 0.3762178122997284  and gradient norm 0.07581852376461029\n",
       "[Iteration. 0] Lost funciton at this iteration 0.3408527672290802  and gradient norm 0.04587741196155548\n",
       "[Iteration. 5] Lost funciton at this iteration 0.33158451318740845  and gradient norm 0.03843647614121437\n",
       "[Iteration. 10] Lost funciton at this iteration 0.3239428400993347  and gradient norm 0.03257639706134796\n",
       "[Iteration. 15] Lost funciton at this iteration 0.31773948669433594  and gradient norm 0.028391094878315926\n",
       "[Iteration. 20] Lost funciton at this iteration 0.312810480594635  and gradient norm 0.025993244722485542\n",
       "[Iteration. 25] Lost funciton at this iteration 0.30901283025741577  and gradient norm 0.025357605889439583\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75993f4ffdd479a88128bf294e355d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#Setting the parameter of this run, we will use optimization nomeclature not ML one.\n",
    "itmax = 100; # Number of epoch.\n",
    "tol = 1e-4\n",
    "step_size = 0.2; #Learning rate\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "H =  Hessian(Loss,x)\n",
    "grad = H.grad().numpy();\n",
    "q = grad\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.constant(step_size,dtype=np.float32)*tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    if it%50 == 0:\n",
    "        print(\"[Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,Loss(x,H.comm),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    H =  Hessian(Loss,x)\n",
    "    grad = H.grad().numpy();\n",
    "    q = grad #H.pCG(grad,10,2,tol=1e-3,itmax=10);\n",
    "    \n",
    "itmax = 200\n",
    "for it in tqdm(range(itmax)):\n",
    "    x = x - tf.Variable(q,dtype=np.float32);\n",
    "    x =  tf.Variable(x)\n",
    "    if it%5 == 0:\n",
    "        print(\"[Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,Loss(x,H.comm),np.linalg.norm(grad)));\n",
    "    if np.linalg.norm(grad)<tol:\n",
    "        break\n",
    "    U, s, Vt = H.shift(x,{\"comp\":SingularCompression,\"rk\":1})\n",
    "    grad = H.grad().numpy();\n",
    "    H.Hs = H.comm.gather(H.memH, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        Hm = (1/len(H.Hs))*np.sum(H.Hs,0);\n",
    "        H.memH = Hm;\n",
    "        H.mem = (H.memH+step_size*(Vt.transpose()@np.linalg.inv(np.diag(s))@U.transpose()));\n",
    "        q = H.memH@grad;\n",
    "        H.comm.bcast(q,root=0)\n",
    "    else:\n",
    "        H.Hs = None\n",
    "        H.mem = (H.memH+step_size*(Vt.transpose()@np.linalg.inv(np.diag(s))@U.transpose()));\n",
    "print(\"Lost funciton at this iteration {}  and gradient norm {}\".format(Loss(x,H.comm),np.linalg.norm(grad)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
