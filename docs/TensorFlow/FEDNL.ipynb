{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdf9204",
   "metadata": {},
   "source": [
    "## Federated Newton Learn\n",
    "In this section we present the algorithm named Federated Newton Learning (FEDNL) introduced in [2].\n",
    "\n",
    "| Flavour | tol | iteration | $$\\mathcal{L}(x_{50})-\\mathcal{L}(x^*)$$ |\n",
    "| :--- | --- | --- | --- |\n",
    "| Vanilla Newton | 1e-4 | 4 | 0.0 |\n",
    "| Rank 1 Compression | 1e-4 | 10 |1.4901161193847656e-07|\n",
    "| Rank 1 Compression Diagonal Regularisation Identity Initial Hessian | 1e-4 | >50 | 0.1078076362609863|\n",
    "| Rank 1 Compression Diagonal Regularisation Null Initial Hessian | 1e-4 | >50 |8.705258369445801e-05|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda80be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a679ea",
   "metadata": {},
   "source": [
    "### Vanilla Newton\n",
    "First we reimplement the vanilla Newton method in the framework of FEDNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed472dee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b190b4972784358a11e88a6a794ec7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0]   8%|▊         | 4/50 [00:16<03:12,  4.18s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1]   8%|▊         | 4/50 [00:16<03:12,  4.18s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.33691510558128357, gradient norm 0.020152254030108452 and error 0.0.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 1] Lost funciton at this iteration 0.3603573739528656  and gradient norm 0.15029960870742798\n",
       "(FedNL) [Iteration. 2] Lost funciton at this iteration 0.3375653922557831  and gradient norm 0.019174691289663315\n",
       "(FedNL) [Iteration. 3] Lost funciton at this iteration 0.3369176983833313  and gradient norm 0.0008387075504288077\n",
       "(FedNL) [Iteration. 4] Lost funciton at this iteration 0.33691510558128357  and gradient norm 4.160287971899379e-06\n",
       "Lost funciton at this iteration 0.33691510558128357, gradient norm 0.02014993503689766 and error 0.0.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x)#,start=0*np.identity(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":119,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm; #A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%1 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02937f",
   "metadata": {},
   "source": [
    "### FEDNL Rank 1 Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb64355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace1ebc2943d40d3aa9196253752f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1]  20%|██        | 10/50 [00:35<02:23,  3.59s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0]  20%|██        | 10/50 [00:35<02:23,  3.59s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.3369152545928955, gradient norm 0.02014790289103985 and error 1.4901161193847656e-07.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 1] Lost funciton at this iteration 0.3603573739528656  and gradient norm 0.15029960870742798\n",
       "(FedNL) [Iteration. 2] Lost funciton at this iteration 0.3409968614578247  and gradient norm 0.01779988408088684\n",
       "(FedNL) [Iteration. 3] Lost funciton at this iteration 0.3384902775287628  and gradient norm 0.007210684008896351\n",
       "(FedNL) [Iteration. 4] Lost funciton at this iteration 0.33764347434043884  and gradient norm 0.004843716975301504\n",
       "(FedNL) [Iteration. 5] Lost funciton at this iteration 0.33708077669143677  and gradient norm 0.0022912921849638224\n",
       "(FedNL) [Iteration. 6] Lost funciton at this iteration 0.3369518518447876  and gradient norm 0.0010489925043657422\n",
       "(FedNL) [Iteration. 7] Lost funciton at this iteration 0.3369239270687103  and gradient norm 0.0004203339631203562\n",
       "(FedNL) [Iteration. 8] Lost funciton at this iteration 0.3369181156158447  and gradient norm 0.0002461163676343858\n",
       "(FedNL) [Iteration. 9] Lost funciton at this iteration 0.3369161784648895  and gradient norm 0.00012412497017066926\n",
       "(FedNL) [Iteration. 10] Lost funciton at this iteration 0.3369154632091522  and gradient norm 7.398983871098608e-05\n",
       "Lost funciton at this iteration 0.3369152545928955, gradient norm 0.020166713744401932 and error 1.4901161193847656e-07.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x)#,start=0*np.identity(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":1,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm; #A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%1 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1d1c0",
   "metadata": {},
   "source": [
    "### FEDNL With Diagonal Regularisation And Different Initial Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ece8d946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:1] 100%|██████████| 50/50 [02:41<00:00,  3.22s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] 100%|██████████| 50/50 [02:41<00:00,  3.22s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 1] Lost funciton at this iteration 1.1105742454528809  and gradient norm 1.2621196508407593\n",
       "(FedNL) [Iteration. 2] Lost funciton at this iteration 0.9810763001441956  and gradient norm 1.1298807859420776\n",
       "(FedNL) [Iteration. 3] Lost funciton at this iteration 0.8773987889289856  and gradient norm 0.9992635250091553\n",
       "(FedNL) [Iteration. 4] Lost funciton at this iteration 0.7962596416473389  and gradient norm 0.8766423463821411\n",
       "(FedNL) [Iteration. 5] Lost funciton at this iteration 0.7336673140525818  and gradient norm 0.7660326957702637\n",
       "(FedNL) [Iteration. 6] Lost funciton at this iteration 0.6856869459152222  and gradient norm 0.6691128015518188\n",
       "(FedNL) [Iteration. 7] Lost funciton at this iteration 0.6488898992538452  and gradient norm 0.5858495235443115\n",
       "(FedNL) [Iteration. 8] Lost funciton at this iteration 0.6205071806907654  and gradient norm 0.5152245759963989\n",
       "(FedNL) [Iteration. 9] Lost funciton at this iteration 0.5984044671058655  and gradient norm 0.45578882098197937\n",
       "(FedNL) [Iteration. 10] Lost funciton at this iteration 0.5809792280197144  and gradient norm 0.4060024321079254\n",
       "(FedNL) [Iteration. 11] Lost funciton at this iteration 0.5670456290245056  and gradient norm 0.3644101619720459\n",
       "(FedNL) [Iteration. 12] Lost funciton at this iteration 0.5557304620742798  and gradient norm 0.32971298694610596\n",
       "(FedNL) [Iteration. 13] Lost funciton at this iteration 0.5463916659355164  and gradient norm 0.3007863759994507\n",
       "(FedNL) [Iteration. 14] Lost funciton at this iteration 0.5385552644729614  and gradient norm 0.27667051553726196\n",
       "(FedNL) [Iteration. 15] Lost funciton at this iteration 0.5318702459335327  and gradient norm 0.2565530240535736\n",
       "(FedNL) [Iteration. 16] Lost funciton at this iteration 0.5260751247406006  and gradient norm 0.23974861204624176\n",
       "(FedNL) [Iteration. 17] Lost funciton at this iteration 0.5209734439849854  and gradient norm 0.22568076848983765\n",
       "(FedNL) [Iteration. 18] Lost funciton at this iteration 0.5164177417755127  and gradient norm 0.21386578679084778\n",
       "(FedNL) [Iteration. 19] Lost funciton at this iteration 0.5122954845428467  and gradient norm 0.20389944314956665\n",
       "(FedNL) [Iteration. 20] Lost funciton at this iteration 0.5085211396217346  and gradient norm 0.19544516503810883\n",
       "(FedNL) [Iteration. 21] Lost funciton at this iteration 0.5050288438796997  and gradient norm 0.18822410702705383\n",
       "(FedNL) [Iteration. 22] Lost funciton at this iteration 0.5017679333686829  and gradient norm 0.1820065677165985\n",
       "(FedNL) [Iteration. 23] Lost funciton at this iteration 0.4986990988254547  and gradient norm 0.17660430073738098\n",
       "(FedNL) [Iteration. 24] Lost funciton at this iteration 0.49579140543937683  and gradient norm 0.17186371982097626\n",
       "(FedNL) [Iteration. 25] Lost funciton at this iteration 0.4930209219455719  and gradient norm 0.16765998303890228\n",
       "(FedNL) [Iteration. 26] Lost funciton at this iteration 0.4903685748577118  and gradient norm 0.1638922095298767\n",
       "(FedNL) [Iteration. 27] Lost funciton at this iteration 0.48781922459602356  and gradient norm 0.16047896444797516\n",
       "(FedNL) [Iteration. 28] Lost funciton at this iteration 0.48536092042922974  and gradient norm 0.15735475718975067\n",
       "(FedNL) [Iteration. 29] Lost funciton at this iteration 0.4829840064048767  and gradient norm 0.15446706116199493\n",
       "(FedNL) [Iteration. 30] Lost funciton at this iteration 0.4806806147098541  and gradient norm 0.15177369117736816\n",
       "(FedNL) [Iteration. 31] Lost funciton at this iteration 0.4784443974494934  and gradient norm 0.14924095571041107\n",
       "(FedNL) [Iteration. 32] Lost funciton at this iteration 0.47627004981040955  and gradient norm 0.14684191346168518\n",
       "(FedNL) [Iteration. 33] Lost funciton at this iteration 0.4741533100605011  and gradient norm 0.1445549875497818\n",
       "(FedNL) [Iteration. 34] Lost funciton at this iteration 0.4720904231071472  and gradient norm 0.14236290752887726\n",
       "(FedNL) [Iteration. 35] Lost funciton at this iteration 0.4700784385204315  and gradient norm 0.1402519941329956\n",
       "(FedNL) [Iteration. 36] Lost funciton at this iteration 0.4681144952774048  and gradient norm 0.1382111757993698\n",
       "(FedNL) [Iteration. 37] Lost funciton at this iteration 0.46619653701782227  and gradient norm 0.13623164594173431\n",
       "(FedNL) [Iteration. 38] Lost funciton at this iteration 0.4643222689628601  and gradient norm 0.13430634140968323\n",
       "(FedNL) [Iteration. 39] Lost funciton at this iteration 0.46249011158943176  and gradient norm 0.1324295997619629\n",
       "(FedNL) [Iteration. 40] Lost funciton at this iteration 0.4606982171535492  and gradient norm 0.13059678673744202\n",
       "(FedNL) [Iteration. 41] Lost funciton at this iteration 0.45894524455070496  and gradient norm 0.1288042515516281\n",
       "(FedNL) [Iteration. 42] Lost funciton at this iteration 0.4572299122810364  and gradient norm 0.12704899907112122\n",
       "(FedNL) [Iteration. 43] Lost funciton at this iteration 0.455550879240036  and gradient norm 0.12532854080200195\n",
       "(FedNL) [Iteration. 44] Lost funciton at this iteration 0.4539068937301636  and gradient norm 0.12364086508750916\n",
       "(FedNL) [Iteration. 45] Lost funciton at this iteration 0.4522969722747803  and gradient norm 0.12198429554700851\n",
       "(FedNL) [Iteration. 46] Lost funciton at this iteration 0.4507199823856354  and gradient norm 0.1203574389219284\n",
       "(FedNL) [Iteration. 47] Lost funciton at this iteration 0.44917505979537964  and gradient norm 0.11875907331705093\n",
       "(FedNL) [Iteration. 48] Lost funciton at this iteration 0.44766107201576233  and gradient norm 0.11718819290399551\n",
       "(FedNL) [Iteration. 49] Lost funciton at this iteration 0.44617727398872375  and gradient norm 0.11564391106367111\n",
       "Lost funciton at this iteration 0.4447227418422699, gradient norm 0.10724713653326035 and error 0.10780763626098633.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.4447227418422699, gradient norm 0.1290498971939087 and error 0.10780763626098633.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617e102fb37a40008df7c40e31126762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x,start=np.eye(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":1,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%1 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3afb8c",
   "metadata": {},
   "source": [
    "### Federated Quasi Newton Learn\n",
    "Here is proposed a version of the Federated Newton Learn algorithm that is based on the idea behind the quasi Newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5ee3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b76f70c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:0] 100%|██████████| 1000/1000 [02:16<00:00,  7.31it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] 100%|██████████| 1000/1000 [02:16<00:00,  7.31it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] (FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 100] Lost funciton at this iteration 0.3436719477176666  and gradient norm 0.009452635422348976\n",
       "(FedNL) [Iteration. 200] Lost funciton at this iteration 0.3393349051475525  and gradient norm 0.004487854428589344\n",
       "(FedNL) [Iteration. 300] Lost funciton at this iteration 0.3380618095397949  and gradient norm 0.002786097815260291\n",
       "(FedNL) [Iteration. 400] Lost funciton at this iteration 0.33752137422561646  and gradient norm 0.0019087998662143946\n",
       "(FedNL) [Iteration. 500] Lost funciton at this iteration 0.33725517988204956  and gradient norm 0.0013744196621701121\n",
       "(FedNL) [Iteration. 600] Lost funciton at this iteration 0.3371131122112274  and gradient norm 0.0010197331430390477\n",
       "(FedNL) [Iteration. 700] Lost funciton at this iteration 0.33703339099884033  and gradient norm 0.0007718248525634408\n",
       "(FedNL) [Iteration. 800] Lost funciton at this iteration 0.33698707818984985  and gradient norm 0.0005924733122810721\n",
       "(FedNL) [Iteration. 900] Lost funciton at this iteration 0.33695951104164124  and gradient norm 0.0004595163627527654\n",
       "Lost funciton at this iteration 0.33694279193878174, gradient norm 0.0201896782964468 and error 2.7686357498168945e-05.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c76b5c24cc4502a974d4cc44e279bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Lost funciton at this iteration 0.33694279193878174, gradient norm 0.020146198570728302 and error 2.7686357498168945e-05.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "from copy import copy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 1000\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "N = 119;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x, opt={\"type\":\"act\"})\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "QInv = np.identity(N);\n",
    "\n",
    "#print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt = H.shift(x,{\"comp\":ActHalko,\"rk\":1,\"type\":\"act\"});\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = copy(H.vecprod);\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    ShiftUs = H.comm.gather(U, root=0);\n",
    "    ShiftVs = H.comm.gather(sigma[0]*Vt, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        u = (1/len(ShiftUs))*np.sum(ShiftUs,0);\n",
    "        v = (1/len(ShiftVs))*np.sum(ShiftVs,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        #SHERMAN-MORRISON\n",
    "        normal = (1+v@QInv@u);\n",
    "        #print(\"Normalisation: \",normal);\n",
    "        A = QInv@u@v@QInv;\n",
    "        #print(\"A Shape: \",A.shape)\n",
    "        QInv = QInv - (1/(1+normal))*A;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        q =  QInv@Grad;\n",
    "        #print(\"Found search dir, \",q.shape);\n",
    "        if it%100 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
