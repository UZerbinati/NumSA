{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdf9204",
   "metadata": {},
   "source": [
    "## Federated Newton Learn\n",
    "In this section we present the algorithm named Federated Newton Learning (FEDNL) introduced in [2].\n",
    "\n",
    "| Flavour | tol | iteration | $$\\mathcal{L}(x_{50})-\\mathcal{L}(x^*)$$ |\n",
    "| :--- | --- | --- | --- |\n",
    "| Vanilla Newton | 1e-4 | 4 | 0.0 |\n",
    "| Rank 1 Compression | 1e-4 | 10 |1.4901161193847656e-07|\n",
    "| Rank 1 Compression Diagonal Regularisation Identity Initial Hessian | 1e-4 | >50 | 0.1078076362609863|\n",
    "| Rank 1 Compression Diagonal Regularisation Null Initial Hessian | 1e-4 | >50 |8.705258369445801e-05|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda80be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a679ea",
   "metadata": {},
   "source": [
    "### Vanilla Newton\n",
    "First we reimplement the vanilla Newton method in the framework of FEDNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed472dee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccc69e651884dc7a537ca1273613436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0]   8%|▊         | 4/50 [00:18<03:29,  4.55s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1]   8%|▊         | 4/50 [00:18<03:29,  4.55s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "Lost funciton at this iteration 0.33691510558128357, gradient norm 0.02014993503689766 and error 0.0.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.33691510558128357, gradient norm 0.020152254030108452 and error 0.0.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x)#,start=0*np.identity(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":119,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm; #A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%25 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02937f",
   "metadata": {},
   "source": [
    "### FEDNL Rank 1 Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb64355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31efcec5bd174180a903335b877df8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0]  20%|██        | 10/50 [00:37<02:30,  3.76s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1]  20%|██        | 10/50 [00:37<02:30,  3.76s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.3369152545928955, gradient norm 0.02014790289103985 and error 1.4901161193847656e-07.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "Lost funciton at this iteration 0.3369152545928955, gradient norm 0.020166713744401932 and error 1.4901161193847656e-07.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x)#,start=0*np.identity(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":1,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm; #A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%25 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1d1c0",
   "metadata": {},
   "source": [
    "### FEDNL With Diagonal Regularisation And Different Initial Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece8d946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:1] 100%|██████████| 50/50 [02:58<00:00,  3.58s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] 100%|██████████| 50/50 [02:58<00:00,  3.58s/it]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] The master Hessian has been initialised\n",
       "Lost funciton at this iteration 0.4447227418422699, gradient norm 0.1290498971939087 and error 0.10780763626098633.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] The master Hessian has been initialised\n",
       "(FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 25] Lost funciton at this iteration 0.4930209219455719  and gradient norm 0.16765998303890228\n",
       "Lost funciton at this iteration 0.4447227418422699, gradient norm 0.10724713653326035 and error 0.10780763626098633.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f053d0f895b479dac4b8e19cafe97f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 50\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x,start=np.eye(x.numpy().shape[0])) #We initialize the shifter\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "Hs = H.comm.gather(H.memH, root=0);\n",
    "if H.comm.Get_rank()==0:\n",
    "    Hm = (1/len(Hs))*np.sum(Hs,0);\n",
    "else:\n",
    "    Hm = None\n",
    "print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt,ell = H.shift(x,{\"comp\":MatSVDCompDiag,\"rk\":1,\"type\":\"mat\"});\n",
    "    shift = Vt.transpose()@np.diag(sigma)@U.transpose();\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = H.memH+step_size*shift;\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    Shifts = H.comm.gather(shift, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    Ells = H.comm.gather(ell, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        Shift = (1/len(Shifts))*np.sum(Shifts,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        Ell = (1/len(Ells))*np.sum(Ells,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        Hm = Hm + step_size*Shift;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        A = Hm + Ell*np.identity(Hm.shape[0]);\n",
    "        q = np.linalg.solve(A,Grad);\n",
    "        #print(\"Found search dir, \",q);\n",
    "        if it%25 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0186b5",
   "metadata": {},
   "source": [
    "### Federated Quasi Newton Learn\n",
    "Here is proposed a version of the Federated Newton Learn algorithm that is based on the idea behind the quasi Newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1030f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b446ec34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206c467943a0426281070d62154b8ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] 100%|██████████| 1000/1000 [02:17<00:00,  7.26it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] 100%|██████████| 1000/1000 [02:17<00:00,  7.27it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] (FedNL) [Iteration. 0] Lost funciton at this iteration 1.2675940990447998  and gradient norm 1.388305902481079\n",
       "(FedNL) [Iteration. 250] Lost funciton at this iteration 0.33856454491615295  and gradient norm 0.0034854577388614416\n",
       "(FedNL) [Iteration. 500] Lost funciton at this iteration 0.3372589945793152  and gradient norm 0.001383052789606154\n",
       "(FedNL) [Iteration. 750] Lost funciton at this iteration 0.33700793981552124  and gradient norm 0.000678437645547092\n",
       "Lost funciton at this iteration 0.33694297075271606, gradient norm 0.02018948644399643 and error 2.7865171432495117e-05.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Lost funciton at this iteration 0.33694297075271606, gradient norm 0.020146775990724564 and error 2.7865171432495117e-05.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numsa.TFHessian import *\n",
    "import dsdl\n",
    "from copy import copy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "ds = dsdl.load(\"a1a\")\n",
    "\n",
    "X, Y = ds.get_train()\n",
    "indx = np.array_split(range(X.shape[0]),int(comm.Get_size()));\n",
    "tfX = []\n",
    "tfY = []\n",
    "for k in range(len(indx)):\n",
    "    tfX = tfX + [tf.sparse.from_dense(np.array(X[indx[comm.Get_rank()]].todense(), dtype=np.float32))]\n",
    "    tfY = tfY + [tf.convert_to_tensor(np.array(Y[indx[comm.Get_rank()]], dtype=np.float32).reshape(X[indx[comm.Get_rank()]].shape[0], 1))]\n",
    "\n",
    "tfXs = tf.sparse.from_dense(np.array(X.todense(), dtype=np.float32))\n",
    "tfYs = tf.convert_to_tensor(np.array(Y, dtype=np.float32).reshape(X.shape[0], 1))\n",
    "#Defining the Loss Function\n",
    "def LossSerial(x):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfXs, x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfYs, Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfXs.shape[0]) + lam*tf.norm(x)**2\n",
    "\n",
    "    return S\n",
    "#Defining the Loss Function\n",
    "def Loss(x,comm):\n",
    "    lam = 1e-3; #Regularisation\n",
    "    x = tf.reshape(x, (119, 1))\n",
    "    Z = tf.sparse.sparse_dense_matmul(tfX[comm.Get_rank()], x, adjoint_a=False)\n",
    "    Z = tf.math.multiply(tfY[comm.Get_rank()], Z)\n",
    "    S = tf.reduce_sum(tf.math.log(1 + tf.math.exp(-Z)) / tfX[comm.Get_rank()].shape[0]) + lam*tf.norm(x)**2\n",
    "    return S\n",
    "################! Setting Of The Solver!##################\n",
    "itmax = 1000\n",
    "tol = 1e-4;\n",
    "step_size=1;\n",
    "N = 119;\n",
    "###########################################################\n",
    "x = tf.Variable(0.1*np.ones((119,1),dtype=np.float32))\n",
    "\n",
    "H = Hessian(Loss,x);\n",
    "H.shift(x, opt={\"type\":\"act\"})\n",
    "#We now collect and average the loc Hessians in the master node (rk 0)\n",
    "QInv = np.identity(N);\n",
    "\n",
    "#print(\"The master Hessian has been initialised\")\n",
    "for it in tqdm(range(itmax)):\n",
    "    # Obtaining the compression of the difference between local mat\n",
    "    # and next local mat.\n",
    "    U,sigma,Vt = H.shift(x,{\"comp\":ActHalko,\"rk\":1,\"type\":\"act\"});\n",
    "    #print(\"Updating local Hessian\")\n",
    "    H.memH = copy(H.vecprod);\n",
    "    grad = H.grad().numpy();\n",
    "    #Now we update the master Hessian and perform the Newton method step\n",
    "    ShiftUs = H.comm.gather(U, root=0);\n",
    "    ShiftVs = H.comm.gather(sigma[0]*Vt, root=0);\n",
    "    Grads = H.comm.gather(grad, root=0);\n",
    "    if H.comm.Get_rank() == 0:\n",
    "        #print(\"Computing the avarage of the local shifts and grad ...\")\n",
    "        u = (1/len(ShiftUs))*np.sum(ShiftUs,0);\n",
    "        v = (1/len(ShiftVs))*np.sum(ShiftVs,0);\n",
    "        Grad = (1/len(Grads))*np.sum(Grads,0);\n",
    "        res = np.linalg.norm(Grad);\n",
    "        #print(\"Computing the master Hessian ...\")\n",
    "        #SHERMAN-MORRISON\n",
    "        normal = (1+v@QInv@u);\n",
    "        #print(\"Normalisation: \",normal);\n",
    "        A = QInv@u@v@QInv;\n",
    "        #print(\"A Shape: \",A.shape)\n",
    "        QInv = QInv - (1/(1+normal))*A;\n",
    "        #print(\"Searching new search direction ...\")\n",
    "        q =  QInv@Grad;\n",
    "        #print(\"Found search dir, \",q.shape);\n",
    "        if it%250 == 0:\n",
    "            print(\"(FedNL) [Iteration. {}] Lost funciton at this iteration {}  and gradient norm {}\".format(it,LossSerial(x),np.linalg.norm(Grad)));\n",
    "        x = x - tf.Variable(q,dtype=np.float32);\n",
    "        x =  tf.Variable(x)\n",
    "    else:\n",
    "        res = None\n",
    "    #Distributing the search direction\n",
    "    x = H.comm.bcast(x,root=0)\n",
    "    res = H.comm.bcast(res,root=0)\n",
    "    if res<tol:\n",
    "            break\n",
    "LossStar =  0.33691510558128357;\n",
    "print(\"Lost funciton at this iteration {}, gradient norm {} and error {}.\".format(LossSerial(x),np.linalg.norm(grad),abs(LossSerial(x)-LossStar)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
